# Towards Large-scale Spiking Neural Networks

> A collection of papers and resources related to deep SNN models
>
> With the thriving of Spiking Transfomrers, we hope this survey could be of help in achieving large-scale SNNs. 
>
> The organization refers to our survey [**"Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions"**](https://www.arxiv.org/abs/2409.02111). 
>
> If you find our survey useful for your research, please cite the following paper:

```
@article{hu2024toward,
  title={Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions},
  author={Hu, Yangfan and Zheng, Qian and Li, Guoqi and Tang, Huajin and Pan, Gang},
  journal={arXiv preprint arXiv:2409.02111},
  year={2024}
}
```

# List of Spiking Transformers
<table>
<thead>
  <tr>
    <th align="center" rowspan="2">Category</th>
    <th align="center" rowspan="2">Paper</th>
    <th align="center" rowspan="2">Release Time</th>
    <th align="center" rowspan="2">Reported Model Size (Max)</th>
    <th align="center" rowspan="2">Code Link</th>
    <th align="center" rowspan="2">Venue</th>
    <th align="center" rowspan="2">Group</th>
    <th align="center" rowspan="2">Comment</th>
  </tr>
  <tr>
  </tr>
</thead>
<tbody>
  <tr>
    <td align="center" rowspan="2">Publicly <br>Accessbile</td>
    <td align="center">Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks</td>
    <td align="center">2019/10</td>
    <td align="center">11</td>
    <td align="center"><a href="https://arxiv.org/abs/1910.10683">Paper</a></td>
  </tr>
</tbody>
</table>
